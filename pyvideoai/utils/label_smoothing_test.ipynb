{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3431c782-6478-4f04-a719-07b34372b049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2703)\n",
      "tensor(1.2703)\n",
      "tensor([[0.0600, 0.0600, 0.7600, 0.0600, 0.0600],\n",
      "        [0.0600, 0.7600, 0.0600, 0.0600, 0.0600],\n",
      "        [0.7600, 0.0600, 0.0600, 0.0600, 0.0600]])\n",
      "tensor(1.3883)\n",
      "tensor(1.3883)\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "\"\"\"\n",
    "Kiyoon implementation taken from https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch\n",
    "Edits:\n",
    "    1. Apply official label smoothing formula. With smoothing=a and num_classes=K, y^LS = y(1-a) + a/K. True label becomes something like 0.933 when a=0.1, depending on how many classes you have.\n",
    "        a. The original code implements differently. y^LS = y(1-a) + (1-y)*a/(K-1). True label becomes 0.9 when a=0.1\n",
    "    2. Accepts custom smooth label instead of 1D tensor label. (OneHotCrossEntropyLoss).\n",
    "\"\"\"\n",
    "class OneHotCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean'):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def reduce_loss(self, loss):\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum() \\\n",
    "        if self.reduction == 'sum' else loss\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        log_preds = F.log_softmax(inputs, -1)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            log_preds = log_preds * self.weight.unsqueeze(0)\n",
    "\n",
    "        return self.reduce_loss(-(targets * log_preds).sum(dim=-1))\n",
    "\n",
    "\n",
    "class LabelSmoothCrossEntropyLoss(OneHotCrossEntropyLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def k_one_hot(self, targets:torch.Tensor, n_classes:int, smoothing=0.0):\n",
    "        with torch.no_grad():\n",
    "            targets = torch.empty(size=(targets.size(0), n_classes),\n",
    "                                  device=targets.device) \\\n",
    "                                  .fill_(smoothing /n_classes) \\\n",
    "                                  .scatter_(1, targets.data.unsqueeze(1), 1.-smoothing + smoothing /n_classes)\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        assert 0 <= self.smoothing < 1\n",
    "\n",
    "        targets = self.k_one_hot(targets, inputs.size(-1), self.smoothing)\n",
    "        return super().forward(inputs, targets)\n",
    "\n",
    "\n",
    "\n",
    "# 1. Devin Yang\n",
    "crit = CrossEntropyLoss()\n",
    "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.9, 0.2, 0.2, 1], \n",
    "                             [1, 0.2, 0.7, 0.9, 1]])\n",
    "label = torch.LongTensor([2, 1, 0])\n",
    "onehot_label = torch.FloatTensor([[0., 0., 1., 0., 0.],\n",
    "                             [0., 1., 0., 0., 0.], \n",
    "                             [1., 0, 0, 0, 0]])\n",
    "\n",
    "# Official PyTorch CrossEntropyLoss test with 1D tensor labels.\n",
    "v = crit(Variable(predict),\n",
    "         Variable(label))\n",
    "print(v)\n",
    "\n",
    "# OneHotCrossEntropyLoss test with one-hot labels\n",
    "crit = OneHotCrossEntropyLoss()\n",
    "v = crit(Variable(predict),\n",
    "         Variable(onehot_label))\n",
    "print(v)\n",
    "\n",
    "#\n",
    "smooth_crit = LabelSmoothCrossEntropyLoss(smoothing=0.3)\n",
    "smooth_label = smooth_crit.k_one_hot(label, 5, smoothing=0.3)\n",
    "print(smooth_label)\n",
    "\n",
    "# OneHotCrossEntropyLoss test with custom applied smooth labels\n",
    "v = crit(Variable(predict),\n",
    "         Variable(smooth_label))\n",
    "print(v)\n",
    "\n",
    "# LabelSmoothingCrossEntropyLoss test\n",
    "v = smooth_crit(Variable(predict),\n",
    "         Variable(label))\n",
    "print(v)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
